# 1.1 Holding XRD or Staking (LSUs) weighted by time

- **Type**: Passive
- **Reward Type**: Multiplier
- **Priority**: High
- **Assets**: XRD, LSU
- **Rules**:
  - Calculate multiplier based on total USD value (XRD + LSUs) via S-curve.
    - Config: S-curve parameters ($5k-$100k range, max 3x).
  - User must meet minimum holding duration to qualify for the full multiplier.
    - Config: Min holding duration (e.g., 24h).
  - Enforce eligibility ($50 USD minimum).
    - Config: Min eligibility value.
  - Track balance changes between start and end date.

## Implementation

### a) granularity by accountAddress & activityId

### arguments:

- accountAddress
- activityId
- weekId

pros

- Adding additional account address does not require recalculation of all accounts.
- Failures are isolated to a single account's calculation.
- Allows for easy retries on a per-account basis.

cons

- Requires dispatching potentially millions of individual jobs per week.
- May require fetching shared data (like prices, activity rules) repeatedly across jobs (mitigated by caching).
- Overall processing time might be longer due to job scheduling overhead compared to a single large batch.

### b) granularity by activityId

- activityId
- weekId

pros

- Fetches shared data (rules, prices) only once per activity per week.
- Potentially faster overall completion time if batch processing is optimized.
- Fewer jobs to manage in the queue.

cons

- A single failure during the batch might require reprocessing the entire batch or complex error handling to isolate failed accounts.
- Adding a new account mid-week might be harder to integrate smoothly.
- Requires more memory/resources per worker to handle the batch.

### c) granularity by userId & activity

- userId
- weekId
- activityId

pros

- Fetches user-level data (e.g., list of linked accounts) once for the specific user, week, and activity calculation.
- Consolidates the calculation _for this specific activity_ across all linked accounts belonging to the user into a single job.
- Organizes work logically by user and the specific activity being calculated.

cons

- Job requires logic to first fetch all `accountAddress`es linked to the `userId`.
- Job must then iterate through each linked account, perform the necessary balance checks and calculations (potentially fetching balance history for _each_ linked account), and correctly aggregate the results according to the activity rules (e.g., summing values across accounts _before_ applying S-curve for this activity).
- Error handling becomes more complex if fetching/calculation fails for one linked account but succeeds for others within the same job.
- Still requires multiple jobs per user per week (one for each relevant activity).
- For this specific activity (holding XRD/LSU), which inherently calculates based on balances per `accountAddress`, this granularity adds complexity (fetching linked accounts, aggregating values) compared to the direct per-`accountAddress` approach.

### d) granularity by userId

- userId
- weekId

pros

- Consolidates calculations for **all** relevant activities for a single user within a specific week into one job.
- Significantly reduces the total number of jobs dispatched compared to per-account or per-user-per-activity approaches.
- Fetches user-level data (linked accounts) only once per user per week.

cons

- Creates highly complex jobs that need to fetch rules for multiple activities, manage different calculation types (passive balance checks, active transaction aggregations, multiplier lookups), and potentially interact with balance history and transaction data for all linked accounts.
- Error handling becomes extremely complex. A failure in calculating one activity (e.g., missing price data for an active trading calculation) must not prevent the calculation and storage of results for other activities (like this holding multiplier) for the same user within the same job.
- Requires significant memory and potentially longer runtime per job.
- Debugging failed jobs is much harder due to the interwoven logic.
- Less flexible for retrying only a specific failed activity calculation for a user.

**Decision:** Considering the expert feedback on utilizing Gateway transaction streams directly for balance changes (avoiding a pre-populated `AccountBalanceHistory` table) and the resulting complexity of calculating Time-Weighted Average (TWA) on-the-fly:

- Calculating TWA directly from Gateway data requires fetching a start balance, fetching all balance-changing transactions in the period (using appropriate `/stream/transactions` filters like `balance_change_resources_filter`, potentially combined with `affected_global_entities_filter`), validating resource movements client-side, and reconstructing the timeline. This is computationally intensive.
- **Option (c) or (d)** (`userId` based) would require performing this complex TWA calculation _multiple times_ (once per linked account) within a single job, significantly increasing job complexity, runtime, and error handling fragility.
- **Option (a) `accountAddress & activityId`** scopes the complex TWA-from-Gateway calculation to a _single account_ per job. While the job is more complex than initially assumed, it's more manageable than user-level granularity jobs.
- This necessitates a **two-stage process**: Stage 1 calculates the TWA USD value per account (this job), and Stage 2 (a separate job/process) aggregates these per-account values for a user, applies the S-curve to the sum, and stores the final multiplier.

Therefore, revert to **granularity by accountAddress & activityId (Option a)** as the most pragmatic starting point. It isolates the complex Gateway interaction and TWA calculation. The subsequent aggregation step is logically separate and simpler.

### Stage 1: Processing Flow (BullMQ Job for accountAddress & activityId granularity - TWA Calc)

1.  **Initialization & Data Fetching:**

    - Receive job: `{ accountAddress, activityId, weekId }`.
    - Fetch `Activity` rules using `activityId` (S-curve params, min duration, eligibility value, LSU addresses).
    - Fetch `Week` details using `weekId` (`startDate`, `endDate`). Map these dates to corresponding Ledger State Versions (or use timestamps if supported by filters).
    - Fetch `UserAccount` record for `accountAddress` to get `userId` and check `is_excluded`. If excluded, terminate successfully (result is effectively 0).
    - Fetch end-of-week USD prices for XRD and relevant LSUs via Price Oracle API (e.g., CoinGecko), utilize caching.

2.  **Eligibility Check (Current Balance):**

    - Fetch _current_ balances for XRD & LSUs for `accountAddress` (e.g., using `/state/entity/details`).
    - Calculate current total USD value using fetched prices.
    - If value < $50 USD (from rules), store intermediate result indicating 0 value (Step 6) and terminate successfully.

3.  **Fetch Starting Balance:**

    - Fetch the balances of XRD & relevant LSUs for `accountAddress` at the state version _just before_ the week `startDate` (using `at_ledger_state` parameter on state endpoint).
    - Handle cases where the account might not have existed or had 0 balance.

4.  **Fetch Balance-Changing Transactions:**

    - Use `/stream/transactions` endpoint.
    - **Filters:**
      - `kind_filter`: "User"
      - Specify ledger state range corresponding to `week.startDate` and `week.endDate`.
      - Crucially, filter to transactions _potentially_ affecting this account's XRD/LSU balance. This might involve combining filters like:
        - `affected_global_entities_filter`: [`accountAddress`]
        - `manifest_resources_filter`: [XRD_address, ...LSU_addresses]
        - _Expert consultation needed here for the optimal filter combination to capture relevant deposits/withdrawals without excessive noise._
    - **Opt-ins:** Request `receipt_resource_movements` to analyze actual balance changes.
    - Fetch all transactions within the range (handle pagination).

5.  **Calculate TWA USD Value for this Account:**

    - Initialize balance timeline with the starting balance (Step 3).
    - Process fetched transactions chronologically:
      - _Validate_ resource movements in the receipt to confirm a deposit/withdrawal of XRD/LSU occurred _specifically involving_ `accountAddress`.
      - Update the balance timeline based on validated movements.
    - Calculate TWA balance for XRD and each relevant LSU based on the reconstructed timeline.
    - Calculate TWA USD value for _this account_: `account_twa_usd = (TWA_XRD * price_XRD) + sum(TWA_LSUs * price_LSUs)`.
    - _(Optional/Complex)_ Check `min holding duration` based on the timeline. If failed, potentially flag the result or set `account_twa_usd` to 0.

6.  **Store Intermediate Result:**

    - Store the calculated `account_twa_usd` value temporarily, associated with `userId`, `accountAddress`, `activityId`, and `weekId`. (e.g., in a Redis cache with TTL, or a dedicated intermediate DB table like `UserAccountWeeklyActivityValue`). This result will be used by Stage 2.

7.  **Job Finalization:**
    - Log success/result.
    - Acknowledge job completion. This job _does not_ calculate the final multiplier.

### Stage 2: Processing Flow (Separate Job/Trigger - User Aggregation & S-Curve)

1.  **Trigger:** Needs a mechanism to run _after_ all Stage 1 jobs for a specific `userId`, `activityId`, `weekId` are complete (e.g., using BullMQ flows/parent-child jobs, or a periodic check).
2.  **Input:** `{ userId, activityId, weekId }`.
3.  **Fetch Intermediate Results:** Retrieve all stored `account_twa_usd` values for this user, activity, and week from the intermediate storage (Redis/DB table).
4.  **Aggregate:** Sum the `account_twa_usd` values across all linked accounts for the user. `total_twa_usd_value = sum(account_twa_usd values)`. Handle cases where some accounts might have failed Stage 1 (e.g., exclude them or use 0).
5.  **Apply S-Curve:** Fetch `Activity.rules`. Apply the `total_twa_usd_value` to the S-curve function. Cap at max multiplier. Result is `calculated_multiplier`.
6.  **Store Final Multiplier:**
    - Find or create `UserWeeklyMultipliers` record for (`userId`, `weekId`).
    - Update the `activityMultipliers` JSON field: `{ ..., [activityId]: calculated_multiplier, ... }`.
7.  **Cleanup:** Optionally clear the intermediate results from Redis/DB table.

### Dependencies (Stage 1 Job)

- BullMQ Client/Worker
- Radix Gateway SDK Client
- Price Oracle Client
- Caching Service Client (e.g., Redis) or DB Client for intermediate results

### Database Interactions (Stage 1 Job)

- **Read:** `Activity`, `Week`, `UserAccount`
- **Write/Update:** Intermediate storage (e.g., Redis cache or `UserAccountWeeklyActivityValue` table)

### Error Handling (Stage 1 Job)

- Define behavior if Gateway calls fail (retries).
- Handle errors during TWA calculation (log, potentially store error state in intermediate result).
- Handle missing price data.
- Gracefully handle accounts below eligibility threshold.

### Comment from GW expert

```
well i'd strongly advise against querying db directly

all features you've mentioned are implemented for /stream/transactions

resulted in a balance change (either a deposit or withdrawal) for XRD or LSU tokens.
balance_change_resources_filter -> that can be used to filter all transactions that resulted in xrd/lsu balance change
You'd have to mix that with affected_global_entities_filter for given account.
Important note is that these 2 filter combination would give you all transactions that resulted in balance change and changed state of given account. It's not guaranteed that balance change was applied to that account. You'd need some extra filtering on frontend or we'd need to change/add extra filters in gateway
between within a specific state version range (e.g., between state version X and state version Y).
possible with at_ledger_state and from_ledger_state

it's also possible to do that with deposit/withdraw events_filter

we'd have to analyze what exactly you need as events/balance changes/manifest would result in different things

i.e if there is withdraw from account and deposit of same amount in same tx it'd publish events but there would be no balance change

if we use manifest as source of truth then it's possible that dApp or sthn else does indirect deposit/withdrawal (as a result of some method call on dApp)

there are many more edge cases

for wallet tx filtering we spent many hours analyzing and preparing requirements
```
