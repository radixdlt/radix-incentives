# Queue Workers Implementation Plan

## 1. Overview

This document outlines the implementation plan for the background queue workers using BullMQ. These workers will be responsible for processing various asynchronous tasks essential for the Radix Incentives Campaign platform, such as calculating activity points/multipliers, aggregating user data, and performing scheduled maintenance. The implementation includes robust job tracking via a dedicated `JobLog` database table.

## 2. Technology Stack

- **Queue System:** BullMQ
- **Message Broker/Store:** Redis
- **Runtime:** Node.js
- **Language:** TypeScript
- **Database Interaction:** Drizzle ORM (PostgreSQL)

## 3. Worker & Queue Structure

- **Queues:** Define logical queues based on task types or priority. Examples:
  - `activity-processing`: Handles individual activity calculations (e.g., the Stage 1 TWA calculation for holdings). High volume.
  - `aggregation`: Handles tasks that aggregate results (e.g., the Stage 2 S-Curve calculation, weekly point summaries). Lower volume, potentially longer running.
  - `transaction-parsing`: Handles parsing incoming blockchain transactions.
  - `default`: For general or low-priority tasks.
- **Workers:**
  - Implement worker processes that listen to specific queues.
  - Use named processors within workers to route jobs to the correct handler function based on the `jobName` provided when the job was added.
  - Design workers to be scalable horizontally (multiple instances can listen to the same queue).
- **Queue Scheduler:** Implement a `QueueScheduler` instance for each queue to handle delayed jobs and stalled job checks.

## 4. Job Processing Logic

- **Job Definition:** Each distinct task will have a defined `jobName` (e.g., `calculate-holding-twa`, `parse-transaction`, `aggregate-user-multiplier`).
- **Idempotency:**
  - Jobs will be added with a custom, deterministic `jobId` generated by the application based on the job's logical identity (e.g., hash of `jobName` + key arguments like `accountAddress`, `weekId`).
  - This `jobId` will be used by BullMQ for duplicate prevention (if the job isn't removed).
- **Processor Functions (using Effect-TS):** Implement handlers corresponding to each `jobName`. These handlers should be constructed using **Effect-TS**:
  - The main handler will likely take the job object (`{ jobId, jobName, data }`) as input and return an `Effect<void, Error>` (or a more specific error type).
  - Utilize **functional composition** (e.g., using `Effect.pipe`) to chain together the steps of the task: fetching data (from Gateway, DB, Price Oracle), performing calculations (TWA, S-curve), aggregating results, etc. Each step would typically be its own Effect.
  - Leverage Effect-TS features for robust error handling (typed errors, error channels), resource management (e.g., database connections, API clients using `Effect.acquireRelease`), concurrency control, and dependency injection.
  - The final composed Effect represents the entire job logic. The worker runner will execute this Effect.
  - Return values can be handled within the Effect pipeline if needed for intermediate steps, but the final result might often be `void` if the primary outcome is database updates.

## 5. Job Logging (`JobLog`) Integration

- **Global Event Listeners:** Attach global listeners (`waiting`, `active`, `completed`, `failed`, `stalled`, `progress` - if progress reporting is added later) to each `Queue` instance.
- **Database Updates:**
  - `waiting`: Create a new `JobLog` entry, storing the custom `jobId`, `queueName`, `jobName`, `jobArguments`, `triggerSource` (extracted from args or default 'system'), `triggeredByAdminUserId` (if applicable). Set status to 'pending'.
  - `active`: Find log by `jobId` & `queueName`, update `status` to 'active', set `startedAt`.
  - `completed`: Find log, update `status` to 'completed', set `endedAt`.
  - `failed`: Find log, update `status` to 'failed', set `endedAt`, store `error_message` and `error_stacktrace`.
  - `stalled`: Find log, potentially update `status` to 'stalled' or handle based on retry logic. Log this occurrence.
- **Correlation:** Use the `jobId` (the custom one provided) from the event object to find and update the correct row in the `JobLog` table.

## 6. Configuration

- **Redis Connection:** Centralized configuration, likely using environment variables (`REDIS_URL`, `REDIS_HOST`, `REDIS_PORT`, `REDIS_PASSWORD`). Ensure robust connection handling and reuse.
- **Worker Concurrency:** Configure the number of concurrent jobs each worker instance can process (`WorkerOptions.concurrency`).
- **Job Options (Defaults):** Define default job options like `attempts` (for retries) and `backoff` strategy (e.g., exponential) for failed jobs.
- **Rate Limiting:** Configure if specific jobs need rate limiting (e.g., external API calls).
- **Job Removal:** Configure `defaultJobOptions` for queues regarding `removeOnComplete` and `removeOnFail`. Recommendation:
  - `removeOnComplete: { count: 1000 }` (Keep a reasonable history of recent successes).
  - `removeOnFail: { count: 10000 }` (Keep a larger number of failed jobs for debugging and potential manual retry via the admin dashboard). _Note the implication for `jobId` uniqueness if many jobs fail and are kept._

## 7. Error Handling Strategy

- **Processor Errors:** Wrap core logic in processor functions with `try...catch`. Log caught errors to the `JobLog` table via the `failed` event listener. Ensure errors are re-thrown so BullMQ marks the job as failed.
- **Retries:** Leverage BullMQ's built-in retry mechanism (`attempts`, `backoff`).
- **Stalled Jobs:** Monitor the `stalled` event. Log these events critically, as they may indicate double processing or event loop blockage. Use `WorkerOptions.lockDuration` and `maxStalledCount` appropriately.
- **Dead Letter Queue (Future):** Consider implementing a pattern where jobs failing consistently after all attempts are moved to a separate "dead-letter" queue for manual inspection, rather than just staying in the 'failed' state indefinitely.
- **Connection Errors:** Implement robust handling for Redis connection errors in workers and listeners.

## 8. Dependencies

- `bullmq`: Core library for queues and workers.
- `ioredis`: Recommended Redis client.
- `effect`: Core Effect-TS library (or specific packages like `@effect/core`, `@effect/io`, etc.)
- Database Client (`drizzle-orm`/`postgres`).
- Configuration management library (e.g., `dotenv`).
- Logging library (e.g., `pino`, `winston`).

## 9. Deployment & Monitoring

- **Deployment:**
  - Run worker processes separately from the main API/frontend applications.
  - Use process managers (like PM2) or container orchestration (Docker/Kubernetes) to manage worker instances.
- **Monitoring:**
  - **Admin Dashboard:** The primary interface for monitoring job status, history, failures, and triggering retries via the `JobLog` table.
  - **Queue Monitoring Tools:** Consider deploying tools like BullBoard or Arenas for real-time queue visualization (job counts per state, worker status).
  - **Logging:** Centralized logging for worker process errors (stdout/stderr, file logs, or logging service).
  - **Redis Monitoring:** Monitor Redis health and memory usage.

## 10. Scalability

- Workers are designed to be stateless (relying on job data and DB).
- Scale horizontally by launching more instances of the worker process listening to the same queue(s). BullMQ handles distributing jobs among available workers.
- Monitor Redis performance as it can become a bottleneck under very high load.
